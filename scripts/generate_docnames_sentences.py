"""
Script to generate DocNames sentences from a Medicare record data sample.
"""
from argparse import ArgumentParser
import csv
from dataclasses import asdict, dataclass
import json
import logging
from math import ceil
from pathlib import Path
import random
import string
from typing import Any, Iterator, Optional, Sequence, TypedDict

import torch
from transformers import AutoTokenizer, PreTrainedTokenizer, pipeline
from transformers.trainer_utils import set_seed

from scripts.utils.medicare_data import (
    firstname_feature,
    lastname_feature,
    specialty_feature,
    legalname_feature,
    city_feature,
    zip_feature,
    addr_feature,
)
from scripts.utils.docnames_data import PromptedSentence

logger = logging.getLogger(__name__)


DEFAULT_SEED = 42
DEFAULT_N_SENTENCES = 1
DEFAULT_PERCENT_WITH_TYPOS = 25
DEFAULT_TEMPERATURE = 1.5

PROMPT_TEMPLATE = string.Template(
    """Write a tweet from a patient's perspective about a doctor including all of the following information:

$features

$extra_instructions"""
)
typo_instructions = "Please introduce exactly one typo when copying this information. You may typo only the included information, such as the doctor's name, and not in any other word. Typos could include missing or transposed letters. For example, you might typo Dr. Smith as Dr. Smiht or as Dr. Smit. Spell every word correctly except for the one word you choose to typo."
final_instructions = "Please write the tweet alone with no additional commentary."

FEATURE_GLOSS = {
    specialty_feature: "Specialty",
    legalname_feature: "Business Name",
    city_feature: "Business City",
    zip_feature: "Business ZIP Code",
    addr_feature: "Business Address",
}
GLOSS_ORDER = ("Doctor", "Specialty", "Business Name", "Business City", "Business ZIP Code", "Business Address")


def load_rows(rows_path: Path) -> Sequence[dict[str, Any]]:
    with rows_path.open(mode="r", encoding="utf-8") as rows_in:
        result = list(csv.DictReader(rows_in))

    for row in result:
        assert lastname_feature in row

    return result


def compute_name(features: dict[str, Any]) -> str:
    """Compute the doctor name (including title) from the given feature values.

    Assumes that the last name is always present."""
    name_parts = ["Dr."]
    if firstname_feature in features:
        name_parts.append(features[firstname_feature])
    name_parts.append(features[lastname_feature])
    result = " ".join(name_parts)
    return result


def format_features(features: dict[str, Any]) -> str:
    """Format the given features dict as a newline-delimited string."""
    gloss_to_value = {
        **{"Doctor": compute_name(features)},
        **{
            FEATURE_GLOSS[name]: value for name, value in features.items()
            if name in FEATURE_GLOSS
        },
    }
    gloss_to_value = dict(sorted(gloss_to_value.items(), key=lambda t: GLOSS_ORDER.index(t[0])))
    logger.debug(
        "Unused features: %s",
        [
            feature
            for feature in features
            if feature not in FEATURE_GLOSS and feature not in [firstname_feature, lastname_feature]
        ],
    )

    result = "\n".join(": ".join([gloss, value]) for gloss, value in gloss_to_value.items())
    return result


def subset_features(features: dict[str, Any], *, rng: random.Random) -> dict[str, Any]:
    """Choose a random subset of features from the given features."""
    # Always include *at a minimum* the doctor's last name.
    result = {lastname_feature: features[lastname_feature]}

    sample_from = features.copy()
    sample_from.pop(lastname_feature)

    sample_size = random.randint(0, len(sample_from))
    sampled_features = random.sample(list(sample_from.items()), k=sample_size)
    result = {**result, **dict(sampled_features)}

    return result


def get_clean_text(generated_text: str) -> str:
    """
    Clean up a tweet generated by the language model.

    This code is tailored to Gemma 2 9B IT and to our current prompt and
    inference hyperparameters.
    """
    result = generated_text.strip()
    lines = [line for line in result.splitlines()]
    if lines:
        result = lines[0]
        if lines[0].startswith("@") and len(lines) >= 3 and len(lines[2]) > len(lines[0]):
            result = lines[2]
    return result


def make_prompt(features: dict[str, Any], *, should_typo: bool, tokenizer: PreTrainedTokenizer) -> str:
    """
    Generate an appropriate prompt using the given features, tokenizer, and other options.
    """
    feature_str = format_features(features)

    extra_instructions = [final_instructions]
    if should_typo:
        extra_instructions.insert(0, typo_instructions)
    extra_instructions_str = " ".join(extra_instructions)

    result = PROMPT_TEMPLATE.substitute(features=feature_str, extra_instructions=extra_instructions_str)
    return result


def write_sentences(prompted_sentences: Iterator[PromptedSentence], sentences_path: Path) -> int:
    """Write the list of sentences to the path as JSONL, returning the number written."""
    result = 0
    with sentences_path.open(mode="w", encoding="utf-8") as jsonl_out:
        for prompted_sentence in prompted_sentences:
            result += 1
            jsonl_out.write(json.dumps(asdict(prompted_sentence)))
            jsonl_out.write("\n")
    return result


def main():
    parser = ArgumentParser(description=__doc__)
    parser.add_argument(
        "sample_path",
        type=Path,
        help="Path to the sample rows file to load.",
    )
    parser.add_argument(
        "sentences_path",
        type=Path,
        help="Where to save the file of genreated sentences.",
    )
    parser.add_argument(
        "--model",
        type=str,
        help="The Hugging Face model to load.",
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        help="The seed used to sample the dataset rows.",
        default=DEFAULT_SEED,
    )
    parser.add_argument(
        "--sentences-per-row",
        type=int,
        help="The number of sentences to generate per row.",
        default=DEFAULT_N_SENTENCES,
    )
    parser.add_argument(
        "--percent-to-typo",
        type=int,
        help="The percent of sentences we should generate with typos.",
        default=DEFAULT_PERCENT_WITH_TYPOS,
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        help="The batch size to use when sampling sentences.",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        help="The temperature to use to sample sentences.",
        default=DEFAULT_TEMPERATURE,
    )
    parser.add_argument(
        "--logging-level",
        type=str,
        default="INFO",
        help="Logging level to use.",
    )
    args = parser.parse_args()

    logging.basicConfig(
        level=getattr(logging, args.logging_level),
        format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    sample_path = args.sample_path
    sentences_path = args.sentences_path
    model = args.model
    random_seed = args.random_seed
    sentences_per_row = args.sentences_per_row
    percent_to_typo = args.percent_to_typo
    batch_size = args.batch_size
    temperature = args.temperature

    tokenizer = AutoTokenizer.from_pretrained(model)
    pipeline_ = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_new_tokens=128, device_map="auto")
    logger.info("Successfully loaded tokenizer and model (model on device %s).", pipeline_.device)

    set_seed(random_seed)
    rng = random.Random(random_seed)

    rows = load_rows(sample_path)
    logger.info("Loaded %d rows from %s", len(rows), sample_path)
    n_to_typo = ceil(len(rows) * percent_to_typo / 100)
    to_typo = set(random.sample(range(len(rows)), k=n_to_typo))
    logger.info(
        "Planning to typo outputs for %d rows (%d sentences) of %d rows (%d sentences) total.",
        len(to_typo),
        len(rows),
        len(to_typo) * sentences_per_row,
        len(rows) * sentences_per_row,
    )
    feature_subsets = [subset_features(row, rng=rng) for row in rows]
    should_typos = [i in to_typo for i in range(len(feature_subsets))]
    nochat_prompts = [
        make_prompt(feature_subset, should_typo=should_typo, tokenizer=tokenizer)
        for feature_subset, should_typo in zip(feature_subsets, should_typos)
    ]
    prompts = [
        tokenizer.apply_chat_template([{"role": "user", "content": prompt}], tokenize=False)
        for raw_prompt in nochat_prompts
    ] if tokenizer.chat_template else nochat_prompts.copy()
    docnames_sentences = (
        PromptedSentence(
            sentence=get_clean_text(response["generated_text"]),
            raw_generation=response["generated_text"],
            prompt=prompt,
            nochat_prompt=nochat_prompt,
            generation_features=features,
            full_features=full_features,
            attempted_to_typo=i in to_typo,
        )
        for i, (nochat_prompt, prompt, full_features, features, responses) in enumerate(zip(nochat_prompts, prompts, rows, feature_subsets, pipeline_(
            prompts,
            num_return_sequences=sentences_per_row,
            do_sample=True,
            return_full_text=False,
            batch_size=batch_size,
            temperature=temperature,
        )))
        for response in responses
    )

    n_sentences = write_sentences(docnames_sentences, sentences_path)
    logger.info("Wrote %s sentences to %s", n_sentences, sentences_path)



if __name__ == "__main__":
    main()
